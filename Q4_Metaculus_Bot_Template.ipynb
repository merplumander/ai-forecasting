{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merplumander/ai-forecasting/blob/metaculus-bot/Q4_Metaculus_Bot_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYpZSIrPA18B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Forecasting Bot Template\n",
        "\n",
        "Instructions for getting this colab running are here: https://www.notion.so/metaculus/Instructions-Resources-for-Bot-Building-in-Q4-8995fc6f18034f52af8d9b084936a2b5?pvs=4#1216aaf4f1018027aaebd8d469d10e07\n",
        "\n",
        "This is a simple bot template that you can use to forecast in the Q4 Metaculus AI Benchmarking Contest. It is a single shot LLM prompt that you are encouraged to experiment with!\n",
        "\n",
        "In order to run this notebook as is, you'll need to enter a few API keys (use the key icon on the left to input them):\n",
        "\n",
        "- `METACULUS_TOKEN`: you can find your Metaculus token under your bot's user settings page: https://www.metaculus.com/accounts/settings/, or on the bot registration page where you created the account: https://www.metaculus.com/aib/\n",
        "- `ASKNEWS_CLIENT_ID` and `ASKNEWS_SECRET` - used to search for relevant news articles from [AskNews](https://asknews.app). Sign up for a [**free account**](https://my.asknews.app/en/plans) and get the secrets from https://my.asknews.app/en/settings/api-credentials\n",
        "  Here is a Metaculus promo code for the Pro tier, which covers all calls for 500 tournament questions: METACULUSBMSERIESQ4\n",
        "\n",
        "Full disclosure: AskNews is also entering a bot into the Q4 competition.\n",
        "\n",
        "Some common alternatives to AskNews include: Perplexity.ai, Exa.ai, or Tavily.\n",
        "The prompt using in this notebook (when combined with Perplexity research summary and Gpt4o) was used for the original Metaculus Bot in Q3 (mf-bot-1 got 4th place)"
      ],
      "metadata": {
        "id": "h51A5J20j3_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "First we need to install necessary dependencys, load our API keys, define our GPT prompt, and set up some helper functions."
      ],
      "metadata": {
        "id": "ViyuUbhIlRyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "wgXS5TENo2OI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tAUPQylJneJK"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai asknews"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load API keys from secrets"
      ],
      "metadata": {
        "id": "tMgMXKX2kxwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have set these in the sidebar to the left, by pressing the key icon.\n",
        "from google.colab import userdata\n",
        "METACULUS_TOKEN = userdata.get('METACULUS_TOKEN')\n",
        "# PERPLEXITY_API_KEY = userdata.get('PERPLEXITY_API_KEY')\n"
      ],
      "metadata": {
        "id": "tADbibLLk68W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Prompt\n",
        "\n",
        "You can change the prompt below to experiment. Key parameters that you can include in your prompt are:\n",
        "\n",
        "*   `{title}` The question itself\n",
        "*   `{background}` The background section of the Metaculus question. This comes from the `description` field on the question\n",
        "*   `{resolution_criteria}` The resolution criteria section of the question\n",
        "*   `{fine_print}` The fine print section of the question\n",
        "*   `{today}` Today's date. Remember that your bot doesn't know the date unless you tell it explicitly!\n",
        "*   `{summary_report}` This is a summary of news articles from AskNews and optionally Perplexity. This is not provided by Metaculus directly, rather the code below calls AskNews by default. In order for this to work you must sign-up for an account as explained at the top of this document.\n",
        "\n",
        "\n",
        "**IMPORTANT**: As you experiment with changing the prompt, be aware that the last number output by the LLM will be used as the forecast probability. The last line in the template specifies that."
      ],
      "metadata": {
        "id": "klYtnLFNlKRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a professional forecaster interviewing for a job.\n",
        "\n",
        "Your interview question is:\n",
        "{title}\n",
        "\n",
        "Background:\n",
        "{background}\n",
        "\n",
        "{resolution_criteria}\n",
        "\n",
        "{fine_print}\n",
        "\n",
        "\n",
        "Your research assistant says:\n",
        "{summary_report}\n",
        "\n",
        "Today is {today}.\n",
        "\n",
        "Before answering you write:\n",
        "(a) The time left until the outcome to the question is known.\n",
        "(b) What the outcome would be if nothing changed.\n",
        "(c) What you would forecast if there was only a quarter of the time left.\n",
        "(d) What you would forecast if there was 4x the time left.\n",
        "\n",
        "You write your rationale and then the last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x3n2kYsZlbxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "This section sets up some simple helper code you can use to get data about forecasting questions and to submit a prediction"
      ],
      "metadata": {
        "id": "9vUab93epmsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "from asknews_sdk import AskNewsSDK\n",
        "import textwrap\n",
        "\n",
        "AUTH_HEADERS = {\"headers\": {\"Authorization\": f\"Token {METACULUS_TOKEN}\"}}\n",
        "API_BASE_URL = \"https://www.metaculus.com/api2\"\n",
        "\n",
        "TOURNAMENT_ID = 32506 # 32506 is the tournament ID for Q4 AI Benchmarking\n",
        "\n",
        "\n",
        "def post_question_comment(question_id: int, comment_text: str) -> None:\n",
        "    \"\"\"\n",
        "    Post a comment on the question page as the bot user.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.post(\n",
        "        f\"{API_BASE_URL}/comments/\",\n",
        "        json={\n",
        "            \"comment_text\": comment_text,\n",
        "            \"submit_type\": \"N\",\n",
        "            \"include_latest_prediction\": True,\n",
        "            \"question\": question_id,\n",
        "        },\n",
        "        **AUTH_HEADERS,\n",
        "    )\n",
        "    if not response.ok:\n",
        "        raise Exception(response.text)\n",
        "\n",
        "\n",
        "def post_question_prediction(question_id: int, prediction_percentage: float) -> None:\n",
        "    \"\"\"\n",
        "    Post a prediction value (between 1 and 100) on the question.\n",
        "    \"\"\"\n",
        "    assert 1 <= prediction_percentage <= 100, \"Prediction must be between 1 and 100\"\n",
        "    url = f\"{API_BASE_URL}/questions/{question_id}/predict/\"\n",
        "    response = requests.post(\n",
        "        url,\n",
        "        json={\"prediction\": float(prediction_percentage) / 100},\n",
        "        **AUTH_HEADERS,\n",
        "    )\n",
        "    if not response.ok:\n",
        "        raise Exception(response.text)\n",
        "\n",
        "\n",
        "def get_question_details(question_id: int) -> dict:\n",
        "    \"\"\"\n",
        "    Get all details about a specific question.\n",
        "    \"\"\"\n",
        "    url = f\"{API_BASE_URL}/questions/{question_id}/\"\n",
        "    response = requests.get(\n",
        "        url,\n",
        "        **AUTH_HEADERS,\n",
        "    )\n",
        "    if not response.ok:\n",
        "        raise Exception(response.text)\n",
        "    return json.loads(response.content)\n",
        "\n",
        "\n",
        "def list_questions(tournament_id=TOURNAMENT_ID, offset=0, count=10) -> list[dict]:\n",
        "    \"\"\"\n",
        "    List (all details) {count} questions from the {tournament_id}\n",
        "    \"\"\"\n",
        "    url_qparams = {\n",
        "        \"limit\": count,\n",
        "        \"offset\": offset,\n",
        "        \"has_group\": \"false\",\n",
        "        \"order_by\": \"-activity\",\n",
        "        \"forecast_type\": \"binary\",\n",
        "        \"project\": tournament_id,\n",
        "        \"status\": \"open\",\n",
        "        \"type\": \"forecast\",\n",
        "        \"include_description\": \"true\",\n",
        "    }\n",
        "    url = f\"{API_BASE_URL}/questions/\"\n",
        "    response = requests.get(url, **AUTH_HEADERS, params=url_qparams)\n",
        "    if not response.ok:\n",
        "        raise Exception(response.text)\n",
        "    data = json.loads(response.content)\n",
        "    return data\n",
        "\n",
        "\n",
        "def call_perplexity(query: str) -> str:\n",
        "    PERPLEXITY_API_KEY = userdata.get(\"PERPLEXITY_API_KEY\")\n",
        "    url = \"https://api.perplexity.ai/chat/completions\"\n",
        "    api_key = PERPLEXITY_API_KEY\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"authorization\": f\"Bearer {api_key}\",\n",
        "        \"content-type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": \"llama-3.1-sonar-large-128k-chat\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\", # this is a system prompt designed to guide the perplexity assistant\n",
        "                \"content\": \"\"\"\n",
        "                  You are an assistant to a superforecaster.\n",
        "                  The superforecaster will give you a question they intend to forecast on.\n",
        "                  To be a great assistant, you generate a concise but detailed rundown of the most relevant news, including if the question would resolve Yes or No based on current information.\n",
        "                  You do not produce forecasts yourself.\n",
        "                  \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\", # this is the actual prompt we ask the perplexity assistant to answer\n",
        "                \"content\": query,\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "    response = requests.post(url=url, json=payload, headers=headers)\n",
        "    if not response.ok:\n",
        "        raise Exception(response.text)\n",
        "    content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(\n",
        "        f\"\\n\\nCalled perplexity with:\\n----\\n{json.dumps(payload)}\\n---\\n, and got\\n:\",\n",
        "        content,\n",
        "    )\n",
        "    return content\n",
        "\n",
        "\n",
        "\n",
        "def get_asknews_context(query: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Use the AskNews `news` endpoint to get news context for your query.\n",
        "    The full API reference can be found here: https://docs.asknews.app/en/reference#get-/v1/news/search\n",
        "    \"\"\"\n",
        "    ask = AskNewsSDK(\n",
        "        client_id=ASKNEWS_CLIENT_ID,\n",
        "        client_secret=ASKNEWS_SECRET,\n",
        "        scopes=[\"news\"]\n",
        "    )\n",
        "\n",
        "    # get the latest news related to the query (within the past 48 hours)\n",
        "    hot_response = ask.news.search_news(\n",
        "        query=query, # your natural language query\n",
        "        n_articles=5, # control the number of articles to include in the context, originally 5\n",
        "        return_type=\"both\",\n",
        "        strategy=\"latest news\" # enforces looking at the latest news only\n",
        "    )\n",
        "\n",
        "    # get context from the \"historical\" database that contains a news archive going back to 2023\n",
        "    historical_response = ask.news.search_news(\n",
        "        query=query,\n",
        "        n_articles=20,\n",
        "        return_type=\"both\",\n",
        "        strategy=\"news knowledge\" # looks for relevant news within the past 60 days\n",
        "    )\n",
        "\n",
        "    # you can also specify a time range for your historical search if you want to\n",
        "    # slice your search up periodically.\n",
        "    # now = datetime.datetime.now().timestamp()\n",
        "    # start = (datetime.datetime.now() - datetime.timedelta(days=100)).timestamp()\n",
        "    # historical_response = ask.news.search_news(\n",
        "    #     query=query,\n",
        "    #     n_articles=20,\n",
        "    #     return_type=\"both\",\n",
        "    #     historical=True,\n",
        "    #     start_timestamp=int(start),\n",
        "    #     end_timestamp=int(now)\n",
        "    # )\n",
        "\n",
        "    news_articles_with_full_context = hot_response.as_string + historical_response.as_string\n",
        "    formatted_articles = format_asknews_context(\n",
        "        hot_response.as_dicts, historical_response.as_dicts)\n",
        "    return news_articles_with_full_context, formatted_articles\n",
        "\n",
        "\n",
        "def format_asknews_context(hot_articles: list[dict], historical_articles: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Format the articles for posting to Metaculus.\n",
        "    \"\"\"\n",
        "\n",
        "    formatted_articles = \"Here are the relevant news articles:\\n\\n\"\n",
        "\n",
        "    if hot_articles:\n",
        "      hot_articles = [article.__dict__ for article in hot_articles]\n",
        "      hot_articles = sorted(\n",
        "          hot_articles, key=lambda x: x['pub_date'], reverse=True)\n",
        "\n",
        "      for article in hot_articles:\n",
        "          pub_date = article[\"pub_date\"].strftime(\"%B %d, %Y %I:%M %p\")\n",
        "          formatted_articles += f\"**{article['eng_title']}**\\n{article['summary']}\\nOriginal language: {article['language']}\\nPublish date: {pub_date}\\nSource:[{article['source_id']}]({article['article_url']})\\n\\n\"\n",
        "\n",
        "    if historical_articles:\n",
        "      historical_articles = [article.__dict__ for article in historical_articles]\n",
        "      historical_articles = sorted(\n",
        "          historical_articles, key=lambda x: x['pub_date'], reverse=True)\n",
        "\n",
        "      for article in historical_articles:\n",
        "          pub_date = article[\"pub_date\"].strftime(\"%B %d, %Y %I:%M %p\")\n",
        "          formatted_articles += f\"**{article['eng_title']}**\\n{article['summary']}\\nOriginal language: {article['language']}\\nPublish date: {pub_date}\\nSource:[{article['source_id']}]({article['article_url']})\\n\\n\"\n",
        "\n",
        "    if not hot_articles and not historical_articles:\n",
        "      formatted_articles += \"No articles were found.\\n\\n\"\n",
        "      return formatted_articles\n",
        "\n",
        "    # formatted_articles += f\"*Generated by AI at [AskNews](https://asknews.app), check out the [API](https://docs.asknews.app) for more information*.\"\n",
        "\n",
        "    return formatted_articles\n",
        "\n",
        "\n",
        "def extract_prediction_from_response_as_percentage_not_decimal(forecast_text: str) -> float:\n",
        "    matches = re.findall(r\"(\\d+)%\", forecast_text)\n",
        "    if matches:\n",
        "        # Return the last number found before a '%'\n",
        "        number = int(matches[-1])\n",
        "        number = min(99, max(1, number))  # clamp the number between 1 and 99\n",
        "        return number\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Could not extract prediction from response: {forecast_text}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def get_gpt_prediction(question_details: dict) -> tuple[float,str]:\n",
        "\n",
        "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    title = question_details[\"question\"][\"title\"]\n",
        "    resolution_criteria = question_details[\"question\"][\"resolution_criteria\"]\n",
        "    background = question_details[\"question\"][\"description\"]\n",
        "    fine_print = question_details[\"question\"][\"fine_print\"]\n",
        "\n",
        "    if GET_NEWS == True:\n",
        "      # If you want to use AskNews, use the below\n",
        "      full_article_context, formatted_articles = get_asknews_context(title)\n",
        "      summary_report = formatted_articles\n",
        "\n",
        "      # If you want to use Perplexity, use the below\n",
        "      # summary_report += call_perplexity(title)\n",
        "    else:\n",
        "      summary_report = \"\"\n",
        "\n",
        "    content = PROMPT_TEMPLATE.format(\n",
        "                  title=title,\n",
        "                  today=today,\n",
        "                  background=background,\n",
        "                  resolution_criteria=resolution_criteria,\n",
        "                  fine_print=fine_print,\n",
        "                  summary_report=summary_report\n",
        "              )\n",
        "\n",
        "    PRINT_LLM_PROMPT = True\n",
        "    if PRINT_LLM_PROMPT:\n",
        "      print(f\"\\n\\n--------LLM PROMPT----------\")\n",
        "      print(content)\n",
        "      print(f\"\\n\\n----END LLM PROMPT----\")\n",
        "\n",
        "\n",
        "    result = requests.post(\n",
        "      \"https://www.metaculus.com/proxy/openai/v1/chat/completions/\",\n",
        "      json={\n",
        "          \"model\": \"gpt-4o\",\n",
        "          \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
        "          \"temperature\": 0,\n",
        "      }, headers={\"Authorization\": f\"Token {METACULUS_TOKEN}\"}\n",
        "    ).content\n",
        "    chat_completion = json.loads(result)\n",
        "\n",
        "    rationale = chat_completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "    probability = extract_prediction_from_response_as_percentage_not_decimal(rationale)\n",
        "    comment = f\"Extracted Probability: {probability}%\\n\\nGPT's Answer: {rationale}\\n\\n\\n ######### PROMPT USED TO GENERATE THE RESPONSE ABOVE ######## {content}\\n\\n\"\n",
        "    return probability, comment"
      ],
      "metadata": {
        "id": "TlgZAk1cppgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List open questions\n",
        "\n",
        "Check which questions you can predict on in the Q4 AI Benchmarking tournament."
      ],
      "metadata": {
        "id": "iN72TKpm32P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = list_questions()\n",
        "\n",
        "open_questions_ids = []\n",
        "for question in questions[\"results\"]:\n",
        "  if question[\"status\"] == \"open\":\n",
        "    print(f\"ID: {question['id']}\\nQ: {question['title']}\\nCloses: {question['scheduled_close_time']}\")\n",
        "    open_questions_ids.append(question[\"id\"])"
      ],
      "metadata": {
        "id": "nXixU7ia35uV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b84156-c760-4c4a-8d72-9aedd5e5fd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: 28905\n",
            "Q: Will Tim Walz cease to be Kamala Harriss's running mate before November 1, 2024?\n",
            "Closes: 2024-10-21T14:30:00Z\n",
            "ID: 28902\n",
            "Q: [PRACTICE] Will Donald Trump be jailed or incarcerated before 2030?\n",
            "Closes: 2024-10-21T14:30:00Z\n",
            "ID: 28901\n",
            "Q: [PRACTICE] Will there be a US-China war before 2035?\n",
            "Closes: 2024-10-21T14:30:00Z\n",
            "ID: 28900\n",
            "Q: [PRACTICE] Will Iran possess a nuclear weapon before 2030?\n",
            "Closes: 2024-10-21T14:30:00Z\n",
            "ID: 28895\n",
            "Q: [PRACTICE] Will any peer-reviewed replication attempt before 2025 confirm the discovery of room-temperature and ambient-pressure superconductivity in LK-99?\n",
            "Closes: 2024-10-18T14:30:00Z\n",
            "ID: 28893\n",
            "Q: [PRACTICE] Will there be Human-machine intelligence parity before 2040?\n",
            "Closes: 2024-10-21T14:30:00Z\n",
            "ID: 28892\n",
            "Q: [PRACTICE] Will someone born before 2001 live to be 150?\n",
            "Closes: 2024-10-21T14:30:00Z\n",
            "ID: 28891\n",
            "Q: [PRACTICE] Will humans go extinct before 2100?\n",
            "Closes: 2024-10-21T14:30:00Z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM forecasting"
      ],
      "metadata": {
        "id": "H63PD3pPqkrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUBMIT_PREDICTION = False # set to True to publish your predictions to Metaculus\n",
        "FORECAST_Q4_AIB = False # set to True to forecast Q4 AI Benchmarking.\n",
        "GET_NEWS = False # set to True to enable AskNews after entering ASKNEWS secrets\n",
        "\n",
        "# The list of questions to forecast\n",
        "forecast_questions_ids = []\n",
        "if FORECAST_Q4_AIB == True:\n",
        "  forecast_questions_ids = open_questions_ids\n",
        "else:\n",
        "  forecast_questions_ids = [28830, 28706] # Only include binary questions\n",
        "\n",
        "\n",
        "if GET_NEWS == True:\n",
        "  ASKNEWS_CLIENT_ID = userdata.get('ASKNEWS_CLIENT_ID')\n",
        "  ASKNEWS_SECRET = userdata.get('ASKNEWS_SECRET')\n",
        "\n",
        "\n",
        "for question_id in forecast_questions_ids:\n",
        "\n",
        "  question_details = get_question_details(question_id)\n",
        "\n",
        "  title = question_details[\"question\"][\"title\"]\n",
        "  resolution_criteria = question_details[\"question\"][\"resolution_criteria\"]\n",
        "  background = question_details[\"question\"][\"description\"]\n",
        "  fine_print = question_details[\"question\"][\"fine_print\"]\n",
        "  print(f\"------------------------\\nQuestion: {title}\\n\\nResolution criteria: {resolution_criteria}\\n\\nDescription: {background}\\n\\nFine print: {fine_print}\\n\\n\")\n",
        "\n",
        "  probability, comment = get_gpt_prediction(question_details)\n",
        "\n",
        "  print(f\"\\n\\n------------------LLM RESPONSE------------\\n\\n\")\n",
        "  print(f\"--------------\\nProbability: {probability}\\n\\nComment: {comment}\\n\\n\")\n",
        "  print(f\"\\n\\n------------------END LLM RESPONSE------------\\n\\n\")\n",
        "\n",
        "  if SUBMIT_PREDICTION:\n",
        "      assert probability is not None, \"Unexpected probability format\"\n",
        "      post_question_prediction(question_id, probability)\n",
        "      post_question_comment(question_id, comment)\n"
      ],
      "metadata": {
        "id": "wd1LY0ozqwQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49492f2e-8303-4a1d-d8b2-daa77c98a8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------\n",
            "Question: Will at least 95% of all new road vehicles with 4+ wheels sold in the US in 2075 have SAE Level 5 autonomy?\n",
            "\n",
            "Resolution criteria: This question will resolve as **Yes** if available data at the time suggest that the percentage of new road vehicles sold in the United States, from January 1, 2075 to December 31, 2075, that have SAE level 5 capabilities is equal to or higher than 95%.\n",
            "\n",
            "For the purposes of this question, road vehicles are defined as motorised machines with **a minimum of 4 wheels** designed for use on roads to transport people, goods, or materials. These include cars, trucks, and buses.\n",
            "\n",
            "Data used to resolve this question need not cover all road vehicle sales in the US and Metaculus admins might use their judgement to determine the resolution.\n",
            "\n",
            "Description: SAE International uses a [classification of 6 levels](https://www.sae.org/blog/sae-j3016-update), from 0 to 5, for self-driving capabilities. Level 5 correspond to full automation where the passengers are never requested to take over driving and the car can drive in all areas and conditions. As of October 2024, [no system has achieved level 5 autonomy](https://en.wikipedia.org/wiki/Self-driving_car).\n",
            "\n",
            "*This question is prompted by a [claim by Elon Musk](https://x.com/elonmusk/status/1844263097244189044) that: \"...all transport will be fully autonomous within 50 years.\"*\n",
            "\n",
            "Fine print: In cases of ambiguity, admins may wait for up to 6 months, until July 1, 2076, to resolve the question.\n",
            "\n",
            "Military, fire, emergency, or law enforcement vehicles are excluded for the purposes of this question.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--------LLM PROMPT----------\n",
            "\n",
            "You are a professional forecaster interviewing for a job.\n",
            "\n",
            "Your interview question is:\n",
            "Will at least 95% of all new road vehicles with 4+ wheels sold in the US in 2075 have SAE Level 5 autonomy?\n",
            "\n",
            "Background:\n",
            "SAE International uses a [classification of 6 levels](https://www.sae.org/blog/sae-j3016-update), from 0 to 5, for self-driving capabilities. Level 5 correspond to full automation where the passengers are never requested to take over driving and the car can drive in all areas and conditions. As of October 2024, [no system has achieved level 5 autonomy](https://en.wikipedia.org/wiki/Self-driving_car).\n",
            "\n",
            "*This question is prompted by a [claim by Elon Musk](https://x.com/elonmusk/status/1844263097244189044) that: \"...all transport will be fully autonomous within 50 years.\"*\n",
            "\n",
            "This question will resolve as **Yes** if available data at the time suggest that the percentage of new road vehicles sold in the United States, from January 1, 2075 to December 31, 2075, that have SAE level 5 capabilities is equal to or higher than 95%.\n",
            "\n",
            "For the purposes of this question, road vehicles are defined as motorised machines with **a minimum of 4 wheels** designed for use on roads to transport people, goods, or materials. These include cars, trucks, and buses.\n",
            "\n",
            "Data used to resolve this question need not cover all road vehicle sales in the US and Metaculus admins might use their judgement to determine the resolution.\n",
            "\n",
            "In cases of ambiguity, admins may wait for up to 6 months, until July 1, 2076, to resolve the question.\n",
            "\n",
            "Military, fire, emergency, or law enforcement vehicles are excluded for the purposes of this question.\n",
            "\n",
            "\n",
            "Your research assistant says:\n",
            "\n",
            "\n",
            "Today is 2024-10-17.\n",
            "\n",
            "Before answering you write:\n",
            "(a) The time left until the outcome to the question is known.\n",
            "(b) What the outcome would be if nothing changed.\n",
            "(c) What you would forecast if there was only a quarter of the time left.\n",
            "(d) What you would forecast if there was 4x the time left.\n",
            "\n",
            "You write your rationale and then the last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----END LLM PROMPT----\n",
            "\n",
            "\n",
            "------------------LLM RESPONSE------------\n",
            "\n",
            "\n",
            "--------------\n",
            "Probability: 80\n",
            "\n",
            "Comment: Extracted Probability: 80%\n",
            "\n",
            "GPT's Answer: (a) The time left until the outcome to the question is known is 51 years, as the question pertains to the year 2075 and today is October 17, 2024.\n",
            "\n",
            "(b) If nothing changed from the current state of technology and adoption, the outcome would be \"No,\" as no vehicles currently have SAE Level 5 autonomy.\n",
            "\n",
            "(c) If there was only a quarter of the time left, meaning approximately 12.75 years from now (around 2037), I would forecast a lower probability than the final answer, as significant advancements and widespread adoption of Level 5 autonomy would still be uncertain by that time.\n",
            "\n",
            "(d) If there was 4x the time left, meaning approximately 204 years from now, I would forecast a higher probability than the final answer, as technological advancements and societal changes over such a long period would likely lead to widespread adoption of fully autonomous vehicles.\n",
            "\n",
            "Rationale:\n",
            "\n",
            "1. **Technological Progress**: The development of autonomous vehicle technology has been progressing, but achieving Level 5 autonomy is a complex challenge. It requires not only technological advancements but also regulatory, legal, and infrastructure changes. Over the next 51 years, it is reasonable to expect significant progress in these areas.\n",
            "\n",
            "2. **Market Adoption**: Assuming technological hurdles are overcome, market adoption will depend on consumer acceptance, cost, and the perceived benefits of fully autonomous vehicles. Historical trends in technology adoption suggest that once a technology becomes viable and cost-effective, adoption can be rapid.\n",
            "\n",
            "3. **Regulatory Environment**: Governments and regulatory bodies will play a crucial role in the adoption of Level 5 vehicles. Over the next five decades, it is likely that regulations will evolve to accommodate and encourage the use of fully autonomous vehicles, especially if they are shown to improve safety and efficiency.\n",
            "\n",
            "4. **Economic and Social Factors**: Economic incentives, such as reduced transportation costs and increased productivity, could drive the adoption of Level 5 vehicles. Social factors, including changing attitudes towards car ownership and environmental concerns, may also influence the transition to autonomous vehicles.\n",
            "\n",
            "5. **Historical Analogies**: Looking at historical analogies, such as the adoption of the internet or smartphones, suggests that once a technology reaches a certain level of maturity and utility, widespread adoption can occur within a few decades.\n",
            "\n",
            "Given these considerations, while there are uncertainties, the long time horizon and potential for technological and societal change lead me to assign a relatively high probability to the scenario where at least 95% of new road vehicles sold in the US in 2075 have SAE Level 5 autonomy.\n",
            "\n",
            "Final Answer: \"Probability: 80%\"\n",
            "\n",
            "\n",
            " ######### PROMPT USED TO GENERATE THE RESPONSE ABOVE ######## \n",
            "You are a professional forecaster interviewing for a job.\n",
            "\n",
            "Your interview question is:\n",
            "Will at least 95% of all new road vehicles with 4+ wheels sold in the US in 2075 have SAE Level 5 autonomy?\n",
            "\n",
            "Background:\n",
            "SAE International uses a [classification of 6 levels](https://www.sae.org/blog/sae-j3016-update), from 0 to 5, for self-driving capabilities. Level 5 correspond to full automation where the passengers are never requested to take over driving and the car can drive in all areas and conditions. As of October 2024, [no system has achieved level 5 autonomy](https://en.wikipedia.org/wiki/Self-driving_car).\n",
            "\n",
            "*This question is prompted by a [claim by Elon Musk](https://x.com/elonmusk/status/1844263097244189044) that: \"...all transport will be fully autonomous within 50 years.\"*\n",
            "\n",
            "This question will resolve as **Yes** if available data at the time suggest that the percentage of new road vehicles sold in the United States, from January 1, 2075 to December 31, 2075, that have SAE level 5 capabilities is equal to or higher than 95%.\n",
            "\n",
            "For the purposes of this question, road vehicles are defined as motorised machines with **a minimum of 4 wheels** designed for use on roads to transport people, goods, or materials. These include cars, trucks, and buses.\n",
            "\n",
            "Data used to resolve this question need not cover all road vehicle sales in the US and Metaculus admins might use their judgement to determine the resolution.\n",
            "\n",
            "In cases of ambiguity, admins may wait for up to 6 months, until July 1, 2076, to resolve the question.\n",
            "\n",
            "Military, fire, emergency, or law enforcement vehicles are excluded for the purposes of this question.\n",
            "\n",
            "\n",
            "Your research assistant says:\n",
            "\n",
            "\n",
            "Today is 2024-10-17.\n",
            "\n",
            "Before answering you write:\n",
            "(a) The time left until the outcome to the question is known.\n",
            "(b) What the outcome would be if nothing changed.\n",
            "(c) What you would forecast if there was only a quarter of the time left.\n",
            "(d) What you would forecast if there was 4x the time left.\n",
            "\n",
            "You write your rationale and then the last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------END LLM RESPONSE------------\n",
            "\n",
            "\n",
            "------------------------\n",
            "Question: Will OpenAI's o1 remain the top LLM in all categories of Chatbot Arena on December 30, 2024?\n",
            "\n",
            "Resolution criteria: This questions resolves as **Yes** if a single o1 model has the same or higher rank than all non-o1 models in all of the following 9 categories on [Chatbot Arena](https://lmarena.ai/), on December 30, 2024:\n",
            "\n",
            "- Overall\n",
            "- Overall w/ Style Control\n",
            "- Hard Prompts (Overall)\n",
            "- Hard Prompts (Overall) w/ Style Control\n",
            "- Instruction Following\n",
            "- Coding\n",
            "- Math\n",
            "- Multi-turn\n",
            "- Longer Query\n",
            "\n",
            "If all o1 models have at least one category in which they are worse than a non-o1 model, this question resolves as **No**.\n",
            "\n",
            "Description: [Chatbot Arena](https://lmarena.ai/) (previously hosted on <https://lmsys.org/>) is a [benchmarking platform](https://lmsys.org/blog/2023-05-03-arena/) for large language models (LLMs). It uses an [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system) similar to the one used in chess to rank LLMs by their capabilities. Rankings are based on user ratings of different LLM systems. Besides general rankings, Chatbot Arena has added various more narrow categories, such as Hard Prompts, Instruction Following, or Math.\n",
            "\n",
            "In September 12, 2024, OpenAI [announced](https://openai.com/o1/) o1, a series of models [trained](https://openai.com/index/learning-to-reason-with-llms/) to use chain of thought to solve complex problems. According to OpenAI, o1 models perform significantly better in math and coding competitions, among other domains. As of October 4, 2024, o1-preview is ranked #1 in all Chatbot Arena categories.\n",
            "\n",
            "For further context please see: \n",
            "\n",
            "- [Chatbot Arena: New models & Elo system update](https://lmsys.org/blog/2023-12-07-leaderboard/)\n",
            "- [Does style matter? Disentangling style and substance in Chatbot Arena](https://lmsys.org/blog/2024-08-28-style-control/)\n",
            "- [Introducing Hard Prompts Category in Chatbot Arena](https://lmsys.org/blog/2024-05-17-category-hard/)\n",
            "\n",
            "Fine print: - For the purposes of this question, a model having no rank in a category means it is worse than all models that have a rank.\n",
            "- Other models also being ranked 1 do not affect resolution, as long as a single o1 model is also ranked 1.\n",
            "- An OpenAI model counts as an o1 model if it contains \"o1\" in its name on Chatbot Arena.\n",
            "- Any categories that are no longer available on Chatbot Arena will not affect resolution, as long as they are fewer than 4. If at least 4 of the aforementioned 9 categories are not available, this question will be **annulled**.\n",
            "- Any categories that are renamed but have the same methodology will be considered equivalent for purposes of this question. \n",
            "- As of October 4, 2024, the relevant ranks are presented in both the Overview tab and in the Arena tab, by selecting the relevant category and potentially applying the required filter.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--------LLM PROMPT----------\n",
            "\n",
            "You are a professional forecaster interviewing for a job.\n",
            "\n",
            "Your interview question is:\n",
            "Will OpenAI's o1 remain the top LLM in all categories of Chatbot Arena on December 30, 2024?\n",
            "\n",
            "Background:\n",
            "[Chatbot Arena](https://lmarena.ai/) (previously hosted on <https://lmsys.org/>) is a [benchmarking platform](https://lmsys.org/blog/2023-05-03-arena/) for large language models (LLMs). It uses an [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system) similar to the one used in chess to rank LLMs by their capabilities. Rankings are based on user ratings of different LLM systems. Besides general rankings, Chatbot Arena has added various more narrow categories, such as Hard Prompts, Instruction Following, or Math.\n",
            "\n",
            "In September 12, 2024, OpenAI [announced](https://openai.com/o1/) o1, a series of models [trained](https://openai.com/index/learning-to-reason-with-llms/) to use chain of thought to solve complex problems. According to OpenAI, o1 models perform significantly better in math and coding competitions, among other domains. As of October 4, 2024, o1-preview is ranked #1 in all Chatbot Arena categories.\n",
            "\n",
            "For further context please see: \n",
            "\n",
            "- [Chatbot Arena: New models & Elo system update](https://lmsys.org/blog/2023-12-07-leaderboard/)\n",
            "- [Does style matter? Disentangling style and substance in Chatbot Arena](https://lmsys.org/blog/2024-08-28-style-control/)\n",
            "- [Introducing Hard Prompts Category in Chatbot Arena](https://lmsys.org/blog/2024-05-17-category-hard/)\n",
            "\n",
            "This questions resolves as **Yes** if a single o1 model has the same or higher rank than all non-o1 models in all of the following 9 categories on [Chatbot Arena](https://lmarena.ai/), on December 30, 2024:\n",
            "\n",
            "- Overall\n",
            "- Overall w/ Style Control\n",
            "- Hard Prompts (Overall)\n",
            "- Hard Prompts (Overall) w/ Style Control\n",
            "- Instruction Following\n",
            "- Coding\n",
            "- Math\n",
            "- Multi-turn\n",
            "- Longer Query\n",
            "\n",
            "If all o1 models have at least one category in which they are worse than a non-o1 model, this question resolves as **No**.\n",
            "\n",
            "- For the purposes of this question, a model having no rank in a category means it is worse than all models that have a rank.\n",
            "- Other models also being ranked 1 do not affect resolution, as long as a single o1 model is also ranked 1.\n",
            "- An OpenAI model counts as an o1 model if it contains \"o1\" in its name on Chatbot Arena.\n",
            "- Any categories that are no longer available on Chatbot Arena will not affect resolution, as long as they are fewer than 4. If at least 4 of the aforementioned 9 categories are not available, this question will be **annulled**.\n",
            "- Any categories that are renamed but have the same methodology will be considered equivalent for purposes of this question. \n",
            "- As of October 4, 2024, the relevant ranks are presented in both the Overview tab and in the Arena tab, by selecting the relevant category and potentially applying the required filter.\n",
            "\n",
            "\n",
            "Your research assistant says:\n",
            "\n",
            "\n",
            "Today is 2024-10-17.\n",
            "\n",
            "Before answering you write:\n",
            "(a) The time left until the outcome to the question is known.\n",
            "(b) What the outcome would be if nothing changed.\n",
            "(c) What you would forecast if there was only a quarter of the time left.\n",
            "(d) What you would forecast if there was 4x the time left.\n",
            "\n",
            "You write your rationale and then the last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----END LLM PROMPT----\n",
            "\n",
            "\n",
            "------------------LLM RESPONSE------------\n",
            "\n",
            "\n",
            "--------------\n",
            "Probability: 85\n",
            "\n",
            "Comment: Extracted Probability: 85%\n",
            "\n",
            "GPT's Answer: To address the interview question, let's break down the components:\n",
            "\n",
            "(a) **The time left until the outcome to the question is known:**\n",
            "\n",
            "The current date is October 17, 2024, and the outcome will be known on December 30, 2024. This gives us approximately 2.5 months or about 10 weeks until the outcome is determined.\n",
            "\n",
            "(b) **What the outcome would be if nothing changed:**\n",
            "\n",
            "As of October 4, 2024, the o1-preview model is ranked #1 in all categories on Chatbot Arena. If nothing changes, the outcome would be a \"Yes,\" as the o1 model would remain the top LLM in all categories.\n",
            "\n",
            "(c) **What you would forecast if there was only a quarter of the time left:**\n",
            "\n",
            "If there was only a quarter of the time left, that would mean about 2.5 weeks remaining. Given the current dominance of the o1 model and the relatively short time frame, it is likely that the o1 model would maintain its top position. Therefore, I would forecast a high probability of \"Yes.\"\n",
            "\n",
            "(d) **What you would forecast if there was 4x the time left:**\n",
            "\n",
            "If there were 10 months left instead of 2.5 months, the landscape could change significantly. Competitors might release new models, and user preferences or evaluation criteria might evolve. This would introduce more uncertainty, and I would forecast a lower probability of \"Yes\" compared to the shorter time frame.\n",
            "\n",
            "**Rationale:**\n",
            "\n",
            "1. **Current Dominance:** The o1-preview model is currently ranked #1 in all categories, indicating strong performance across diverse tasks. This suggests a robust model that is well-received by users.\n",
            "\n",
            "2. **Time Frame:** With 2.5 months remaining, there is limited time for significant changes in rankings, especially if the o1 model continues to perform well and OpenAI maintains its competitive edge.\n",
            "\n",
            "3. **Competition:** While competitors may release new models, the time required for these models to gain traction and surpass the o1 model in all categories is relatively short.\n",
            "\n",
            "4. **Technological Advancements:** OpenAI's focus on chain of thought and complex problem-solving suggests that the o1 model is designed to handle a wide range of tasks effectively, which may help it maintain its lead.\n",
            "\n",
            "5. **Market Dynamics:** The LLM market is highly competitive, and rapid advancements are possible. However, the current lead and the short time frame reduce the likelihood of a complete shift in rankings.\n",
            "\n",
            "**Final Answer:**\n",
            "\n",
            "Given the current dominance of the o1 model, the limited time frame, and the competitive landscape, I would assign a high probability to the o1 model remaining the top LLM in all categories by December 30, 2024.\n",
            "\n",
            "Probability: 85%\n",
            "\n",
            "\n",
            " ######### PROMPT USED TO GENERATE THE RESPONSE ABOVE ######## \n",
            "You are a professional forecaster interviewing for a job.\n",
            "\n",
            "Your interview question is:\n",
            "Will OpenAI's o1 remain the top LLM in all categories of Chatbot Arena on December 30, 2024?\n",
            "\n",
            "Background:\n",
            "[Chatbot Arena](https://lmarena.ai/) (previously hosted on <https://lmsys.org/>) is a [benchmarking platform](https://lmsys.org/blog/2023-05-03-arena/) for large language models (LLMs). It uses an [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system) similar to the one used in chess to rank LLMs by their capabilities. Rankings are based on user ratings of different LLM systems. Besides general rankings, Chatbot Arena has added various more narrow categories, such as Hard Prompts, Instruction Following, or Math.\n",
            "\n",
            "In September 12, 2024, OpenAI [announced](https://openai.com/o1/) o1, a series of models [trained](https://openai.com/index/learning-to-reason-with-llms/) to use chain of thought to solve complex problems. According to OpenAI, o1 models perform significantly better in math and coding competitions, among other domains. As of October 4, 2024, o1-preview is ranked #1 in all Chatbot Arena categories.\n",
            "\n",
            "For further context please see: \n",
            "\n",
            "- [Chatbot Arena: New models & Elo system update](https://lmsys.org/blog/2023-12-07-leaderboard/)\n",
            "- [Does style matter? Disentangling style and substance in Chatbot Arena](https://lmsys.org/blog/2024-08-28-style-control/)\n",
            "- [Introducing Hard Prompts Category in Chatbot Arena](https://lmsys.org/blog/2024-05-17-category-hard/)\n",
            "\n",
            "This questions resolves as **Yes** if a single o1 model has the same or higher rank than all non-o1 models in all of the following 9 categories on [Chatbot Arena](https://lmarena.ai/), on December 30, 2024:\n",
            "\n",
            "- Overall\n",
            "- Overall w/ Style Control\n",
            "- Hard Prompts (Overall)\n",
            "- Hard Prompts (Overall) w/ Style Control\n",
            "- Instruction Following\n",
            "- Coding\n",
            "- Math\n",
            "- Multi-turn\n",
            "- Longer Query\n",
            "\n",
            "If all o1 models have at least one category in which they are worse than a non-o1 model, this question resolves as **No**.\n",
            "\n",
            "- For the purposes of this question, a model having no rank in a category means it is worse than all models that have a rank.\n",
            "- Other models also being ranked 1 do not affect resolution, as long as a single o1 model is also ranked 1.\n",
            "- An OpenAI model counts as an o1 model if it contains \"o1\" in its name on Chatbot Arena.\n",
            "- Any categories that are no longer available on Chatbot Arena will not affect resolution, as long as they are fewer than 4. If at least 4 of the aforementioned 9 categories are not available, this question will be **annulled**.\n",
            "- Any categories that are renamed but have the same methodology will be considered equivalent for purposes of this question. \n",
            "- As of October 4, 2024, the relevant ranks are presented in both the Overview tab and in the Arena tab, by selecting the relevant category and potentially applying the required filter.\n",
            "\n",
            "\n",
            "Your research assistant says:\n",
            "\n",
            "\n",
            "Today is 2024-10-17.\n",
            "\n",
            "Before answering you write:\n",
            "(a) The time left until the outcome to the question is known.\n",
            "(b) What the outcome would be if nothing changed.\n",
            "(c) What you would forecast if there was only a quarter of the time left.\n",
            "(d) What you would forecast if there was 4x the time left.\n",
            "\n",
            "You write your rationale and then the last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------END LLM RESPONSE------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Eq8JG_dvAxs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}