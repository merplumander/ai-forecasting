{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment of working with a model's log probabilities as forecasting probabilites\n",
    "\n",
    "## Background\n",
    "During pretraining, LLMs use the cross entropy loss, a proper scoring rule. Therefore they are trained to return their true best guess as to the next token. This is basically a forecasting task. Given what you have seen so far (in your context) and given what you have learned so far (in your weights from previous training examples), predict the next word. You have to build a world model and a model of language etc. to do so well. \n",
    "\n",
    "Now given that LLMs are pretrained this way, can't I elicit an LLMs true belief probability on a binary question, by asking it to return \"yes\" or \"no\" and looking at the log_probs for these tokens?\n",
    "\n",
    "Apparently no. I just get the more likely result with almost certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "context_prompt = \"Respond only with either yes or no by using the following machine readable format surrounding the response with two asteriks: **yes** or **no**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_question = \"I'm throwing a six-sided die. Did it land on the number 1?\"\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"{forecasting_question}\"},\n",
    "        {\"role\": \"system\", \"content\": f\"{context_prompt}\"},\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=20,\n",
    "    max_completion_tokens=5,\n",
    "    temperature=2,\n",
    "    # top_p=0.8,\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    token = response.choices[0].logprobs.content[1].top_logprobs[i].token\n",
    "    prob = np.exp(response.choices[0].logprobs.content[1].top_logprobs[i].logprob)\n",
    "    print(f\"The probability of {token} is {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But somehow the model does argmax here instead of returning faithful probabilities. For anything with a probability lower than 0.5 it says no with almost certainty and for anything higher than a probability of 0.5 it says yes with almost certainty.\n",
    "\n",
    "I tried gpt-4o, gpt-4o-mini, and gpt-3.5-turbo. With the probabilities getting closer to the extremes with more capable models.\n",
    "\n",
    "Changing the temperature does not seem to influence this behaviour.\n",
    "\n",
    "\n",
    "Example questions and responses:\n",
    "```\n",
    "forecasting_question = \"I'm throwing a six-sided die. Did it land on a number lower than 5?\"\n",
    "```\n",
    " response:   \n",
    "> The probability of yes is 0.999948788531352    \n",
    " The probability of unknown is 2.7535031259121803e-05\n",
    "\n",
    "\n",
    "```\n",
    "forecasting_question = \"I'm throwing a six-sided die. Did it land on the number 3?\"\n",
    "```\n",
    "response:   \n",
    ">The probability of no is 0.9999919389784903  \n",
    "The probability of yes is 7.88926171285407e-06 \n",
    "\n",
    "\n",
    "\n",
    "And even when it should be 50/50 the response is often very strong in a random direction (or maybe leaning to no).\n",
    "```\n",
    "forecasting_question = \"I'm throwing a fair coin. Will it show heads?\"\n",
    "```\n",
    "response:  \n",
    ">The probability of no is 0.8519258384620517  \n",
    "The probability of yes is 0.1480425124768776  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ai_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
